# Daily Paper Notes - 2026-02-26

## 今日主题

`Diffusion Language Model (dLLM)` 最新进展：重点看「可扩展训练」「理解-生成平衡」「效率优化」。

> 数据时间：截至 2026-02-26（美国时间）。

## arXiv 最新相关论文

1. **LLaDA 2.1: DLMs are Better Than LLMs at Training Time and Inference Time**  
   - 日期：2026-02（arXiv:2602.08676）  
   - 链接：https://arxiv.org/abs/2602.08676  
   - 摘要：延续 LLaDA 路线，强调 dLLM 在训练与推理两端的综合性能与效率优势。

2. **Balancing Understanding and Generation in Discrete Diffusion Language Models**  
   - 日期：2026-02（arXiv:2602.01362）  
   - 链接：https://arxiv.org/abs/2602.01362  
   - 摘要：讨论 dLLM 在「理解任务 vs 生成任务」之间的能力权衡，并提出改进方法。

3. **FOCUS: Fine-grained Online Cache Update for Efficient Diffusion Language Model Inference**  
   - 日期：2026-01（arXiv:2601.23278）  
   - 链接：https://arxiv.org/abs/2601.23278  
   - 摘要：面向推理效率，提出更细粒度的在线 cache 更新机制以减少冗余计算。

4. **LLaDA 2.0: Scaling up Diffusion Language Models for Efficient Pretraining and Inference**  
   - 日期：2025-12（arXiv:2512.15745）  
   - 链接：https://arxiv.org/abs/2512.15745  
   - 摘要：将 dLLM 扩展到更大规模，系统讨论预训练与推理效率的工程化路径。

## Hugging Face 最新相关内容

1. **Hugging Face Papers: LLaDA 2.1**  
   - 链接：https://huggingface.co/papers/2602.08676  
   - 亮点：社区讨论集中在 dLLM 与传统自回归 LLM 的训练/推理成本差异。

2. **Hugging Face Papers: Balancing Understanding and Generation in dLLM**  
   - 链接：https://huggingface.co/papers/2602.01362  
   - 亮点：关注统一建模下的任务偏置问题，对多任务场景有直接参考价值。

3. **Hugging Face Papers: FOCUS**  
   - 链接：https://huggingface.co/papers/2601.23278  
   - 亮点：工程可落地性高，适合直接迁移到推理系统优化实验。

4. **Hugging Face Models: GSAI-ML/LLaDA-8B-Instruct**  
   - 链接：https://huggingface.co/GSAI-ML/LLaDA-8B-Instruct  
   - 亮点：可用于快速验证 dLLM 指令跟随与生成质量。

5. **Hugging Face Models: GSAI-ML/LLaDA-2.1-mini**  
   - 链接：https://huggingface.co/GSAI-ML/LLaDA-2.1-mini  
   - 亮点：轻量版本适合做小成本复现实验。

## 今日归纳与亮点

1. **规模化能力在增强**：从 LLaDA 2.0 到 2.1，dLLM 不再只是概念验证，开始进入系统化性能竞争阶段。  
2. **研究重点从“能不能做”转向“怎么平衡”**：理解与生成能力权衡成为核心问题。  
3. **推理优化是近期主线**：FOCUS 类方法说明 dLLM 的工程优化空间仍然很大。  
4. **生态可用性提升**：Hugging Face 上已有可直接试验的模型，降低了实践门槛。

## 建议阅读顺序

1. `LLaDA 2.1`（先看最新总体方向）  
2. `Balancing Understanding and Generation...`（理解任务权衡）  
3. `FOCUS`（补齐工程效率）  
4. 回看 `LLaDA 2.0`（理解演进脉络）
